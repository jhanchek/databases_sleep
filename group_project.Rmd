---
title: "Board Game Analysis"
subtitle: "Team Maximus"
author: "Chris Dinkins, Jackson Hanchek, Harrison Peloquin, Hunter Sawyer"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

# Library Installations  

```{r,echo=TRUE,message=FALSE,error=FALSE,warning=FALSE}
library("pander")
library("ggplot2")
library("DT")
library("corrplot")
library("datarium")
bgg_df <- read.csv("./bgg_dataset.csv", sep = ";")
```


# Explanation of Data Source  

```{r}
sub_df <- bgg_df[1:15,]
pander(sub_df)


```


\textcolor{blue}{https://www.kaggle.com/datasets/andrewmvd/board-games?resource=download}

Our dataset is maintained on kaggle.com, a platform for data science related tools and discussion. Our data set, titled “Board Games”, was   uploaded to the site by a senior data scientist from the Hospital Israelita Albert Einstein, the highest prestige hospital in Latin America. This data set, however, does not involve medical data as it was a leisure project. The data set, which contains over twenty thousand different board games, was taken from the website BoardGameGeek in February of 2021. BoardGameGeek is currently the largest board game ranking platform, and it contains many different stats about over one hundred thousand board games. The twenty thousand board games in the data set are a subset of the larger hundred thousand. They are ranked in popularity, which requires user accounts to vote on them. Alongside popularity and name, the data includes publishing year, minimum and maximum player count, play length, suggested age, and rating from a scale of one through ten.



# Initial Observations



When our group first attempted to handle our dataset, we realized the number of rows was so large that our computers had difficulty processing it. In fact, the dataset's size was gargantuan enough to give us issues when trying to create tables using pander. We have decided the best course of action to solve this predicament was to take a smaller portion of the data and use that to represent the dataset. By choosing the first 1,000 entries, we eliminate the bias that would come from handpicking certain data. We used this smaller data set for most testing purposes, but we did end up using the full data set. When we used the full data set we found a number of missing values in the number of owned users. We removed the contaminated observations.
  
After observing our data closely, we have noticed that there are a few cases of missing data. One such case can be found in the 'BoardGamesGeek ID', where board games can be found that do not have any IDs entered at all. We have decided that these null values do not justify throwing the entire rows away as the 'BoardGamesGeek ID' does not directly affect any way that we are analyzing or wanting to use this data. Instead, we have decided to leave these values empty.  
  
  
One of the most prominent predicaments we have come across in the early stages of observing our data is how to handle noisy data. The first instances we noticed were in the 'Min Players' and the 'Max Players' columns. Here, we saw board games hold the value of 0, which seems meaningless and does not make much sense at first.However, considering the context of what the minimum players and the maximum players means, we theorize that a 0 value means the board game did not specify a minimum or a maximum number of players. With that said, we have decided to simply remove these rows from our data sets. This accounted for a very small amount of the data and would not massively effect analysis.
  
We have also seen that the 'Year Published' column has negative values in it, which is completely illogical given the context. At first, we considered that the negative signs were a mistake, and we could take the absolute value. However, values reach -3500 so an absolute value would result in a year that has not yet occurred at the time of observing the data. Perhaps negative values represent years from BC as opposed to AD. This could be the case, but it is difficult to imagine humans in 1300 BC passing time by playing a friendly match of Tic-Tac-Toe, so it would probably be in our best interest to consult a domain expert before making assumptions.  

  
We also noticed that the average rating and average complexity were in decimal format with commas instead of decimal points. This is common practice in Brazil, where this data was collected. We changed the commas to decimals to make performing calculations easier.

The last thing we noticed was that there were some missing/negative values for the number of people who own some of the games. These also accounted for a tiny portion of the data so we simply omit the relevant observations.

```{r,echo=TRUE,warning=FALSE}

#useful categorizations of column names

variables <- colnames(bgg_df)
numeric_variables <- variables[c(3,4,5,6,7,8,9,10,11,12)]
continues_variables <- numeric_variables[c(8,10)]
numeric_variables2 <- variables[c(3,4,5,6,7,8,10,12)]


#print(colnames(bgg_df))
#print(numeric_variables)




#create both data sets with noisy data removed
#removed_zero_playerCount_df is no min or max players of 0
removed_zero_playerCount_df <- subset(bgg_df,(bgg_df$Min.Players != 0 & bgg_df$Max.Players != 0))

#removed_zero_years_df is df with no published year of 0
removed_zero_years_df <- subset(bgg_df,(bgg_df$Year.Published > 0))

removed_both_df <- subset(bgg_df,(bgg_df$Min.Players != 0 & bgg_df$Max.Players != 0 &   
                                  bgg_df$Year.Published > 0 & bgg_df$Owned.Users > 0))
above_2000_df <- na.omit(  
subset(bgg_df,(bgg_df$Min.Players != 0 & bgg_df$Max.Players != 0 & bgg_df$Year.Published > 1990)))



```



# Variable explanations

* ID represents the unique ID each board game is given in the BoardGamesGeek database. This is an integer.
* Name is the name of each board game in the database. This is a string.
* Year Published represents the (approximated in some cases) publishing year of the board game. Note that this is a signed integer, as dates * older than 1 AD are displayed as negatives.
* Min Players is the publisher suggested minimum number of players. This is an integer. Note that some board games display a suggested * minimum players of zero.
* Max Players is the publisher suggested maximum number of players. This is also an integer.
* Play Time is the publisher suggested average play time. If there is a range, it uses the maximum. This is an integer and is in minutes.
* Min Age is the publisher suggested minimum age of play. This is an integer.
* Users Rated is the number of users of BoardGameGeek who have given a rating to the board game. This is an integer.
* Rating Average is the average of all of the individual user ratings of the board game on the site. This is a float with two decimal places.  
  * Note the commas instead of periods to denote decimal points, which is the common standard in Brazil.
* BGG Rank is the ranking of the board game on BoardGameGeek’s “Best of All Time” list. This rank is correlated with Rating Average and Users Rated. This is an integer. Note that each entry is unique, as there can only be one “best game of all time” or “second best game of all time”, etc.
* Complexity is the rating of how hard a game is to learn or understand. It is an average of users' responses to the question "How heavy (difficult/complex) is this game?" on a scale of 1-5, where 1 is "light" and 5 is "heavy." It is an integer.
* Users Owned is a the number of BGG users who have marked the game as part of their library. It is an integer. 


# Histograms

The two histograms below are based upon the BGG rank and the User count respectively.  
The BGG rank histogram is a flat distribution,which is what we would expect because of the fact that BGG ranks are unique to each game and assigned sequentially starting at 1.The histogram for the number of owned users looks like a Pareto distribution. It has the majority of games having player counter below 50,000
and the number of games with a higher player count decreasing as you get higher play counts.
```{r,echo=TRUE}

# histogram creation

for(variable in continues_variables) {
  
  print(
    ggplot(removed_both_df, aes_string(x=variable))
    + geom_histogram(
      colour="darkorchid4", fill="darkorchid1", position="identity", bins=10, alpha=0.2
    )
    + ggtitle(paste("Frequency Distribution (", variable, ")", sep=""))
    + theme(plot.title=element_text(hjust = 0.5))
  )
}


```



```{r, echo=TRUE}

removed_both_df$Complexity.Average <- as.numeric(gsub(",",".",removed_both_df$Complexity.Average))
removed_both_df$Rating.Average <- as.numeric(gsub(",",".",removed_both_df$Rating.Average))


above_2000_df$Complexity.Average <- as.numeric(gsub(",",".",above_2000_df$Complexity.Average))
above_2000_df$Rating.Average <- as.numeric(gsub(",",".",above_2000_df$Rating.Average))

display_df_removed_outliers <- subset(bgg_df,(bgg_df$Min.Players != 0 &   
          bgg_df$Max.Players != 0 &bgg_df$Max.Players < 20 ))


```






# Correlation Analysis 
We created a correlation chart for all of the numeric variables. We have also observed a few correlations that we found interesting upon our initial analysis. For example, we have found a very high   correlation between the ‘Rating Average’ and the ‘BGG Rank’ columns. If we view the board games that are ranked near the top of ‘BGG Rank’,  We can see the associated ‘Rating Average’ values are typically among the highest.   

We also can see that there is a nearly 1 positive correlation for the number of users that own a game, and the number of users that rated a game.
The reason for this is that the data set contains only ranked games. This is likely because of the likelihood that every person who rates a game on BGG, also checks on the website that they own the game.  

We also notice that there is a very positive relation between the play time and the complexity. It seems likely that this is a causal relationship where the more complex a game is, the longer the play time becomes.  


```{r,echo=TRUE}
print(numeric_variables)

correlations <- cor(removed_both_df[numeric_variables[]])
corrplot(
  correlations,
  type="upper",
  tl.col="black",
  tl.srt=45
)

```


# Scatter Plots

```{r,echo=TRUE}
plot(bgg_df$Owned.Users,   
  bgg_df$Users.Rated, type = "p",  xlim = NULL, ylim = NULL,log = "",  
  main = "Users Owned vs Users Rated", sub = NULL, xlab = "Users Owned", ylab = "Users Rated",
ann = par("ann"), axes = TRUE)
```

```{r,echo=TRUE}
plot(removed_both_df$Users.Rated,   
     removed_both_df$Rated.Average, type = "p",  xlim = NULL, ylim = NULL,
log = "", main = "Users Rated vs Average Rating", sub = NULL, xlab = "Users rated",   
ylab = "Average Rating",ann = par("ann"), axes = TRUE)
```


# Box Plots


Note : We made two graphs for the year the games were published in. One with all the data included, and one with all games published before 1990 excluded. We did this so it is easier to see the distribution of publishing dates for the more modern games, but you can still see that there are some outliers way back in history.

```{r, message=FALSE, echo=TRUE,warning=FALSE}

print(
    ggplot(display_df_removed_outliers, aes_string(y="Year.Published"))
    + geom_boxplot(colour="darkorchid4", fill="darkorchid1", alpha=0.2)
  )

display_df_removed_outliers <-    
subset(display_df_removed_outliers,(display_df_removed_outliers$Year.Published >= 1990))

for(var in numeric_variables2){
  group <- "color"
  print(
    ggplot(display_df_removed_outliers, aes_string(y=var))
    + geom_boxplot(colour="darkorchid4", fill="darkorchid1", alpha=0.2)
  )
}
```

# Project Goals

When our group first observed the data, we began to wonder what traits make games good. Are more complex games more desirable, or is simplicity key? Are newer games preferred, or are older games better? Through analysis of this data, we aim to develop an understanding of which traits are desired in a board game.

Our group intends to create two regression models based on the attributes of our data. One model will predict the quality of the game in terms of average user rating, and the other will predict the popularity its popularity in terms of users owned. The coefficients assigned in the regression model will tell us the significance of each attribute in the overall opinion of the game. The use of two regression models generates two separate views. It tells us what traits result in games which are good, and what traits result in games which are popular.


# Regressions

We began by making simple regressions of single variables using *lm*. First, we analyzed the effect of Min_Players on Rating_Average. In this analysis, we find that there is a very weak correlation, with a coefficient of -0.22. In the graph, we can see that the line does not trace the data very well, and has a small R squared value. The fact that the coefficient is negative support the idea that games with a lower minimum requirement of players will be rated higher. This suggests that games which are easier to get a group to play with are rated higher.
```{r}
print(colnames(removed_zero_years_df))

x <- as.numeric(unlist( removed_both_df[c(4)]))
y <- as.numeric(unlist( removed_both_df[c(9)]))

regression <- lm(y~x)

print(summary(regression))

plot(y,x,col = "blue",main = "minimum players and average rating regression",
abline(lm(x~y)),xlab = "average ratings",ylab = "min players ")

```

```{r}
x <- as.numeric(unlist( removed_both_df[c(9)]))
y <- as.numeric(unlist( removed_both_df[c(12)]))

regression <- lm(y~x)

print(summary(regression))

plot(y,x,col = "blue",main = "average ratings and owned users regression",
abline(lm(x~y)),xlab = "owned users",ylab = "average ratings")


```


```{r}
x <- as.numeric(unlist( removed_both_df[c(11)]))
y <- as.numeric(unlist( removed_both_df[c(12)]))

regression <- lm(y~x)

print(summary(regression))

plot(y,x,col = "blue",main = "owned users and average complexity regression",
abline(lm(x~y)),xlab = "owned users",ylab = "average complexity")


```



```{r}
x <- as.numeric(unlist( removed_both_df[c(6)]))
y <- as.numeric(unlist( removed_both_df[c(12)]))

regression <- lm(y~x)

print(summary(regression))

plot(y,x,col = "blue",main = "owned users and play time regression",
abline(lm(x~y)),xlab = "owned users",ylab = "Play time")


```


```{r}
x <- as.numeric(unlist( removed_both_df[c(3)]))
y <- as.numeric(unlist( removed_both_df[c(9)]))

regression <- lm(y~x)

print(summary(regression))

plot(y,x,col = "blue",main = "average ratings and publishing year regression",
abline(lm(x~y)),xlab = "publishing year",ylab = "average ratings")


```


```{r}
x <- as.numeric(unlist( removed_both_df[c(10)]))
y <- as.numeric(unlist( removed_both_df[c(12)]))

regression <- lm(y~x)

print(summary(regression))

plot(y,x,col = "blue",main = "owned users and bgg rank regression",
abline(lm(x~y)),xlab = "owned users",ylab = "bgg rank")


```

# Multiple Regression

When making our multiple regression model, we began by comparing the correlation between variables. There are three major correlations. The first is between BGG.Rank and Rating.Average, which makes sense, since the rank is a function of the average rating. Since the rank is categorical, we will exclude it anyway. The second is between Owned.Users and Users.Rated, which also makes sense. Because owned users is one of our goal targets, we will exclude users rated to not distort our model. Finally, Complexity.Average and Rating.Average have a decent correlation. This is decided to be a sensible correlation which doesn't distort our data but instead gives a view into what makes users rate a game highly, and will not be excluded.

```{r}
dv_df <- read.csv("./dataset_for_viewing.csv", sep = ",")
dv_df_removed <- subset(dv_df,(dv_df$Min.Players != 0 & dv_df$Max.Players != 0 &   
                                  dv_df$Year.Published > 0 & dv_df$Owned.Users > 0))
dv_df_all <- dv_df_removed[c(3,4,5,6,7,8,9,10,11,12)]
dv_df_numeric <- dv_df_removed[c(3,4,5,6,7,9,11,12)]

cor(dv_df_all, y = dv_df_all, method="pearson")
```
Here are the variables we will be working with.
```{r}
head(dv_df_numeric)
cor(dv_df_numeric, y = dv_df_numeric, method="pearson")
```

# Rating Average Model

This is our model for Rating.Average. It outputs the function "Rating.Average = (.003 * (Year.Published)) + (0.114 (Min.Players)) + (.000319 (Max.Players)) + (.0000144 (Play.Time)) + (.0154 (Min.Age)) + (.491 (Complexity.Average)) - .600"

The p-value for our model is very low, indicating that there is a low probability of finding extreme new data outside of our prediction. However, the p-value isn't as low for all of our variables. Play.Time's p-value is higher but still fairly low, but Max.Players' p-value is quite high. This suggests that the maximum recommended player count of a game may be a poor predictor of its rating. Anecdotally, we find that we rarely play games with groups so large that not everyone can play, so it makes sense that maximum player count isn't significant in peoples' ratings.

When we observe the R-Squared value, we see that it is fairly low at 0.26, meaning only about a quarter of the variation in the data can be explained by the model. While this doesn't automatically mean the model as bad, it does suggest that only a small part of the whole story can be explained by these variables, and that there may be significant factors which we haven't investigated.

Another thing to note is that the estimated coefficients are all very low. This can be explained by the domains of these factors. The year published can reach as high as 2022, while the target variable only ranges from 1 to 10. The large difference in domains between variables makes it difficult to get a strong hold on the actual significance of each variable.

We attempted to use this model to predict values for three recent games. Note that a prediction cannot be made using values outside the domain for the model, but since the model includes a game released in 2022, it is fine for us to make predictions on games released in 2022.

The first game was Weather Machine published by Eagle-Gryphon Games in 2022. Plugging in values gives the equation "(.003 * (2022)) + (0.114 (1)) + (.000319 (4)) + (.0000144 (150)) + (.0154 (14)) + (.491 (4.45)) - .600" and the result is 7.984. The actual rating the game received is 7.981.

The second game was Frosthaven, published by Cephalofair Games in 2022. The predicted ownership was found to be 7.596, and the actual rating is 7.227.

The third game was Heat: Pedal to the Metal, published by Days of Wonder. The predictor ownership was calculated to be 6.788. The actual rating is much higher, at 7.860. The correlation analysis earlier in the report reveals a similarity between complexity and rank. The complexity for Heat was fairly low, at 2.14, compared to Weather Machine at 4.45 and Frosthaven at 3.66. With this in mind, it may be that the model over emphasizes complexity when predicting a game's rating.
```{r}
Rating_Model <- lm(Rating.Average ~ Year.Published + Min.Players + Max.Players + Play.Time + Min.Age + Complexity.Average, data = dv_df_numeric)
summary(Rating_Model)
```

# Users Owned Model

This is our model for Owned.Users. It outputs the function "Owned.Users = (-0.216 * (Year.Published)) + (-71.760 (Min.Players)) + (2.06 (Max.Players)) + (-0.170 (Play.Time)) + (85.487 (Min.Age)) + (424.054 (Complexity.Average)) + 337.840"

The p-value for this model is also very low, but this time, many of the individual variables have very high p-values, especially Year.Published. This suggests that the age of a game makes for a terrible predictor of how popular it is. It is likely that the users of BoardGameGeek.com are constantly growing their collection, meaning they have fairly equal access to both more modern games and older games.

The R Squared of this model is abysmally low, meaning our model is very unlikely to accurately predict how popular a game is based on the attributes we investigated.

As with the Rating.Average model, the coefficients of this model are unusual, bit this time, they are often very large. This is because our target variable, Owned.Users, easily reaches values into the tens of thousands, and rarely, hundreds of thousands. Meanwhile, a predictor variable such as Complexity.Average, with a coefficient of 424.054, has a domain of just 1 to 5.

We attempted to use this model to predict values for three recent games. Note that a prediction cannot be made using values outside the domain for the model, but since the model includes a game released in 2022, it is fine for us to make predictions on games released in 2022.

The first game was Weather Machine published by Eagle-Gryphon Games. Plugging in values gives the equation "(-0.216 * (2022)) + (-71.760 (1)) + (2.06 (4)) + (-0.170 (150)) + (85.487 (14)) + (424.054 (4.45)) + 337.840" and the result is 2,895.93. The actual ownership of the game is 2,521.

The second game was Frosthaven, published by Cephalofair Games. The predicted ownership was found to be 2566.02, and the actual ownership was 1,790.

The third game was Heat: Pedal to the Metal, published by Days of Wonder. The predictor ownership was calculated to be 1593.83. The actual ownership was 1,659.

These predictions are closer than one might expect from such an uncertain model, but are still not great. In addition, there is some bias in game choice. Since these games are all very recent releases, they are only owned by a small number of people. The longer a game is around, the more variation there may be in terms of how many people own it.
```{r}
Pop_Model <- lm(Owned.Users ~ Year.Published + Min.Players + Max.Players + Play.Time + Min.Age + Complexity.Average, data = dv_df_numeric)
summary(Pop_Model)
```

# Conclusions

While our models make it difficult to make significant conclusions, there are still certainly some which can be made. For one, even though it's hard to find the weight of coefficients, we can still see their sign. Year.Published is positive in the rating model, which suggests that newer games are likely to be rated higher than older ones. This means modern advances in the board game market are likely favored by fans and consumers, as opposed to preferring the nostalgia of older games. The especially low coefficients and p-values of Max.Players and Play.Time may mean that these factors are relatively unimportant in determining rating. Finally, due to the low r-squared of the model, it is possible that there are new conclusions to be made by further exploring more attributes of the data.

In terms of the ownership model, its low confidence may suggest that these attributes have very little to contribute when predicting how many people will own a game. The popularity of a game may even be inherently random. Another possibility is that these attributes can contribute the a game's popularity, but that Owned.Users is a poor indicator of popularity.

# How to improve the model

We have determined several ways in which we could alter our model to improve its predictive capabilities. Firstly, we would normalize the data in order to get the data to exist within a similar domain. This would make it much easier to understand the coefficients of our regression. Secondly, we may take the year released and subtract it from 2022 to get the game's age. This would make the variable much more intuitive to understand. Finally, we may attempt to include more data. Our data set contains a wealth of categorical information of the mechanics (such as dice rolling or deck construction) and domains (such as "strategy games" or "party games"). These attributes have a huge domain, which makes analyzing them very difficult. However, they have the potential to reveal trends which other attributes cannot. There are many other attributes of games which were not in our data set that may still reveal trends, such as the designer, publisher, or the popularity of an existing franchise that the game may be licensing.

## Citations

  Dilini Samarasinghe, July 5, 2021, "BoardGameGeek Dataset on Board Games", IEEE Dataport, doi: https://dx.doi.org/10.21227/9g61-bs59.